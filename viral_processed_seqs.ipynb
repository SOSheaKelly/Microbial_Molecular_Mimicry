{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processed Viral Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Show and Print Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked list of queries based on the number of hits:\n",
      "1. Query ID: sp|Q02388|CO7A1_HUMAN, Hits: 7362\n",
      "2. Query ID: sp|P19838|NFKB1_HUMAN, Hits: 878\n",
      "3. Query ID: sp|Q15303|ERBB4_HUMAN, Hits: 743\n",
      "4. Query ID: sp|P19525|E2AK2_HUMAN, Hits: 717\n",
      "5. Query ID: sp|Q00534|CDK6_HUMAN, Hits: 582\n",
      "6. Query ID: sp|P06493|CDK1_HUMAN, Hits: 563\n",
      "7. Query ID: sp|Q00526|CDK3_HUMAN, Hits: 560\n",
      "8. Query ID: sp|P24941|CDK2_HUMAN, Hits: 531\n",
      "9. Query ID: sp|P11802|CDK4_HUMAN, Hits: 509\n",
      "10. Query ID: sp|Q00535|CDK5_HUMAN, Hits: 505\n",
      "11. Query ID: sp|Q8NFJ6|PKR2_HUMAN, Hits: 500\n",
      "12. Query ID: sp|P32247|BRS3_HUMAN, Hits: 500\n",
      "13. Query ID: sp|P43490|NAMPT_HUMAN, Hits: 500\n",
      "14. Query ID: sp|P46663|BKRB1_HUMAN, Hits: 500\n",
      "15. Query ID: sp|P01019|ANGT_HUMAN, Hits: 500\n",
      "16. Query ID: sp|P55265|DSRAD_HUMAN, Hits: 459\n",
      "17. Query ID: sp|Q9Y3Z3|SAMH1_HUMAN, Hits: 115\n",
      "18. Query ID: sp|P47898|5HT5A_HUMAN, Hits: 70\n",
      "19. Query ID: sp|P25445|TNR6_HUMAN, Hits: 70\n",
      "20. Query ID: sp|P20591|MX1_HUMAN, Hits: 31\n",
      "21. Query ID: sp|P20592|MX2_HUMAN, Hits: 30\n",
      "22. Query ID: sp|O95786|RIGI_HUMAN, Hits: 18\n",
      "23. Query ID: sp|P38398|BRCA1_HUMAN, Hits: 18\n",
      "24. Query ID: sp|O43808|PM34_HUMAN, Hits: 16\n",
      "25. Query ID: sp|P01344|IGF2_HUMAN, Hits: 14\n",
      "26. Query ID: sp|P55210|CASP7_HUMAN, Hits: 10\n",
      "27. Query ID: sp|Q07812|BAX_HUMAN, Hits: 10\n",
      "28. Query ID: sp|Q16611|BAK_HUMAN, Hits: 6\n",
      "29. Query ID: sp|P06396|GELS_HUMAN, Hits: 6\n",
      "30. Query ID: sp|O95390|GDF11_HUMAN, Hits: 4\n",
      "31. Query ID: sp|O14879|IFIT3_HUMAN, Hits: 4\n",
      "32. Query ID: sp|P60484|PTEN_HUMAN, Hits: 4\n",
      "33. Query ID: sp|Q14790|CASP8_HUMAN, Hits: 3\n",
      "34. Query ID: sp|P42574|CASP3_HUMAN, Hits: 3\n",
      "35. Query ID: sp|P56199|ITA1_HUMAN, Hits: 2\n",
      "36. Query ID: sp|Q7Z2W4|ZCCHV_HUMAN, Hits: 2\n",
      "37. Query ID: sp|Q12965|MYO1E_HUMAN, Hits: 2\n",
      "38. Query ID: sp|P10645|CMGA_HUMAN, Hits: 2\n",
      "39. Query ID: sp|P01966|HBA_BOVIN, Hits: 1\n",
      "40. Query ID: sp|Q14765|STAT4_HUMAN, Hits: 1\n",
      "41. Query ID: sp|O00910|STATA_DICDI, Hits: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Path to the master file\n",
    "master_filepath = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/filtered_viral_master_blaster.txt\"\n",
    "\n",
    "# Read the contents of the master file\n",
    "with open(master_filepath, \"r\") as master_file:\n",
    "    # Read all lines from the file\n",
    "    lines = master_file.readlines()\n",
    "\n",
    "# Extract query sequence IDs from each line\n",
    "query_ids = [line.split()[0] for line in lines if line.strip()]\n",
    "\n",
    "# Count the occurrences of each query ID\n",
    "query_counts = Counter(query_ids)\n",
    "\n",
    "# Sort the queries based on the number of hits (descending order)\n",
    "ranked_queries = sorted(query_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the ranked list of queries\n",
    "print(\"Ranked list of queries based on the number of hits:\")\n",
    "for rank, (query, count) in enumerate(ranked_queries, start=1):\n",
    "    print(f\"{rank}. Query ID: {query}, Hits: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_queries = [(query, count) for query, count in ranked_queries if query != \"#\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of viral hits:\n",
      "Query ID\tHits\n",
      "sp|Q02388|CO7A1_HUMAN\t7362\n",
      "sp|P19838|NFKB1_HUMAN\t878\n",
      "sp|Q15303|ERBB4_HUMAN\t743\n",
      "sp|P19525|E2AK2_HUMAN\t717\n",
      "sp|Q00534|CDK6_HUMAN\t582\n",
      "sp|P06493|CDK1_HUMAN\t563\n",
      "sp|Q00526|CDK3_HUMAN\t560\n",
      "sp|P24941|CDK2_HUMAN\t531\n",
      "sp|P11802|CDK4_HUMAN\t509\n",
      "sp|Q00535|CDK5_HUMAN\t505\n",
      "sp|Q8NFJ6|PKR2_HUMAN\t500\n",
      "sp|P32247|BRS3_HUMAN\t500\n",
      "sp|P43490|NAMPT_HUMAN\t500\n",
      "sp|P46663|BKRB1_HUMAN\t500\n",
      "sp|P01019|ANGT_HUMAN\t500\n",
      "sp|P55265|DSRAD_HUMAN\t459\n",
      "sp|Q9Y3Z3|SAMH1_HUMAN\t115\n",
      "sp|P47898|5HT5A_HUMAN\t70\n",
      "sp|P25445|TNR6_HUMAN\t70\n",
      "sp|P20591|MX1_HUMAN\t31\n",
      "sp|P20592|MX2_HUMAN\t30\n",
      "sp|O95786|RIGI_HUMAN\t18\n",
      "sp|P38398|BRCA1_HUMAN\t18\n",
      "sp|O43808|PM34_HUMAN\t16\n",
      "sp|P01344|IGF2_HUMAN\t14\n",
      "sp|P55210|CASP7_HUMAN\t10\n",
      "sp|Q07812|BAX_HUMAN\t10\n",
      "sp|Q16611|BAK_HUMAN\t6\n",
      "sp|P06396|GELS_HUMAN\t6\n",
      "sp|O95390|GDF11_HUMAN\t4\n",
      "sp|O14879|IFIT3_HUMAN\t4\n",
      "sp|P60484|PTEN_HUMAN\t4\n",
      "sp|Q14790|CASP8_HUMAN\t3\n",
      "sp|P42574|CASP3_HUMAN\t3\n",
      "sp|P56199|ITA1_HUMAN\t2\n",
      "sp|Q7Z2W4|ZCCHV_HUMAN\t2\n",
      "sp|Q12965|MYO1E_HUMAN\t2\n",
      "sp|P10645|CMGA_HUMAN\t2\n",
      "sp|P01966|HBA_BOVIN\t1\n",
      "sp|Q14765|STAT4_HUMAN\t1\n",
      "sp|O00910|STATA_DICDI\t1\n",
      "End of list.\n"
     ]
    }
   ],
   "source": [
    "print(\"List of viral hits:\")\n",
    "print(\"Query ID\\tHits\")  \n",
    "for query, count in filtered_queries:\n",
    "    print(f\"{query}\\t{count}\") \n",
    "\n",
    "print(\"End of list.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked list of Accession IDs based on the number of hits:\n",
      "1. Accession ID: Q02388, Hits: 7362\n",
      "2. Accession ID: P19838, Hits: 878\n",
      "3. Accession ID: Q15303, Hits: 743\n",
      "4. Accession ID: P19525, Hits: 717\n",
      "5. Accession ID: Q00534, Hits: 582\n",
      "6. Accession ID: P06493, Hits: 563\n",
      "7. Accession ID: Q00526, Hits: 560\n",
      "8. Accession ID: P24941, Hits: 531\n",
      "9. Accession ID: P11802, Hits: 509\n",
      "10. Accession ID: Q00535, Hits: 505\n",
      "11. Accession ID: Q8NFJ6, Hits: 500\n",
      "12. Accession ID: P32247, Hits: 500\n",
      "13. Accession ID: P43490, Hits: 500\n",
      "14. Accession ID: P46663, Hits: 500\n",
      "15. Accession ID: P01019, Hits: 500\n",
      "16. Accession ID: P55265, Hits: 459\n",
      "17. Accession ID: Q9Y3Z3, Hits: 115\n",
      "18. Accession ID: P47898, Hits: 70\n",
      "19. Accession ID: P25445, Hits: 70\n",
      "20. Accession ID: P20591, Hits: 31\n",
      "21. Accession ID: P20592, Hits: 30\n",
      "22. Accession ID: O95786, Hits: 18\n",
      "23. Accession ID: P38398, Hits: 18\n",
      "24. Accession ID: O43808, Hits: 16\n",
      "25. Accession ID: P01344, Hits: 14\n",
      "26. Accession ID: P55210, Hits: 10\n",
      "27. Accession ID: Q07812, Hits: 10\n",
      "28. Accession ID: Q16611, Hits: 6\n",
      "29. Accession ID: P06396, Hits: 6\n",
      "30. Accession ID: O95390, Hits: 4\n",
      "31. Accession ID: O14879, Hits: 4\n",
      "32. Accession ID: P60484, Hits: 4\n",
      "33. Accession ID: Q14790, Hits: 3\n",
      "34. Accession ID: P42574, Hits: 3\n",
      "35. Accession ID: P56199, Hits: 2\n",
      "36. Accession ID: Q7Z2W4, Hits: 2\n",
      "37. Accession ID: Q12965, Hits: 2\n",
      "38. Accession ID: P10645, Hits: 2\n",
      "39. Accession ID: P01966, Hits: 1\n",
      "40. Accession ID: Q14765, Hits: 1\n",
      "41. Accession ID: O00910, Hits: 1\n",
      "\n",
      "CSV file with ranked Accession IDs has been saved to /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_hits.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Path to the master file\n",
    "master_filepath = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/filtered_viral_master_blaster.txt\"\n",
    "output_csv_file = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_hits.csv\"\n",
    "\n",
    "# Read the contents of the master file\n",
    "with open(master_filepath, \"r\") as master_file:\n",
    "    # Read all lines from the file\n",
    "    lines = master_file.readlines()\n",
    "\n",
    "# Extract Accession IDs from each line\n",
    "accession_ids = []\n",
    "for line in lines:\n",
    "    if line.strip():\n",
    "        match = re.search(r'\\|([^|]+)\\|', line)\n",
    "        if match:\n",
    "            accession_ids.append(match.group(1))\n",
    "\n",
    "# Count the occurrences of each Accession ID\n",
    "accession_counts = Counter(accession_ids)\n",
    "\n",
    "# Sort the Accession IDs based on the number of hits (descending order)\n",
    "ranked_accessions = sorted(accession_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the ranked list of Accession IDs\n",
    "print(\"Ranked list of Accession IDs based on the number of hits:\")\n",
    "for rank, (accession, count) in enumerate(ranked_accessions, start=1):\n",
    "    print(f\"{rank}. Accession ID: {accession}, Hits: {count}\")\n",
    "\n",
    "# Create a DataFrame from the ranked Accession IDs\n",
    "df = pd.DataFrame(ranked_accessions, columns=[\"Accession ID\", \"Hits\"])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "print(f\"\\nCSV file with ranked Accession IDs has been saved to {output_csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create filepaths for new processed fasta's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FASTA file 'Q02388.fasta' created with hits for Query ID 'Q02388'.\n",
      "FASTA file 'P19838.fasta' created with hits for Query ID 'P19838'.\n",
      "FASTA file 'Q15303.fasta' created with hits for Query ID 'Q15303'.\n",
      "FASTA file 'P19525.fasta' created with hits for Query ID 'P19525'.\n",
      "FASTA file 'Q00534.fasta' created with hits for Query ID 'Q00534'.\n",
      "FASTA file 'P06493.fasta' created with hits for Query ID 'P06493'.\n",
      "FASTA file 'Q00526.fasta' created with hits for Query ID 'Q00526'.\n",
      "FASTA file 'P24941.fasta' created with hits for Query ID 'P24941'.\n",
      "FASTA file 'P11802.fasta' created with hits for Query ID 'P11802'.\n",
      "FASTA file 'Q00535.fasta' created with hits for Query ID 'Q00535'.\n",
      "FASTA file 'Q8NFJ6.fasta' created with hits for Query ID 'Q8NFJ6'.\n",
      "FASTA file 'P32247.fasta' created with hits for Query ID 'P32247'.\n",
      "FASTA file 'P43490.fasta' created with hits for Query ID 'P43490'.\n",
      "FASTA file 'P46663.fasta' created with hits for Query ID 'P46663'.\n",
      "FASTA file 'P01019.fasta' created with hits for Query ID 'P01019'.\n",
      "FASTA file 'P55265.fasta' created with hits for Query ID 'P55265'.\n",
      "FASTA file 'Q9Y3Z3.fasta' created with hits for Query ID 'Q9Y3Z3'.\n",
      "FASTA file 'P47898.fasta' created with hits for Query ID 'P47898'.\n",
      "FASTA file 'P25445.fasta' created with hits for Query ID 'P25445'.\n",
      "FASTA file 'P20591.fasta' created with hits for Query ID 'P20591'.\n",
      "FASTA file 'P20592.fasta' created with hits for Query ID 'P20592'.\n",
      "FASTA file 'O95786.fasta' created with hits for Query ID 'O95786'.\n",
      "FASTA file 'P38398.fasta' created with hits for Query ID 'P38398'.\n",
      "FASTA file 'O43808.fasta' created with hits for Query ID 'O43808'.\n",
      "FASTA file 'P01344.fasta' created with hits for Query ID 'P01344'.\n",
      "FASTA file 'P55210.fasta' created with hits for Query ID 'P55210'.\n",
      "FASTA file 'Q07812.fasta' created with hits for Query ID 'Q07812'.\n",
      "FASTA file 'Q16611.fasta' created with hits for Query ID 'Q16611'.\n",
      "FASTA file 'P06396.fasta' created with hits for Query ID 'P06396'.\n",
      "FASTA file 'O95390.fasta' created with hits for Query ID 'O95390'.\n",
      "FASTA file 'O14879.fasta' created with hits for Query ID 'O14879'.\n",
      "FASTA file 'P60484.fasta' created with hits for Query ID 'P60484'.\n",
      "FASTA file 'Q14790.fasta' created with hits for Query ID 'Q14790'.\n",
      "FASTA file 'P42574.fasta' created with hits for Query ID 'P42574'.\n",
      "FASTA file 'P56199.fasta' created with hits for Query ID 'P56199'.\n",
      "FASTA file 'Q7Z2W4.fasta' created with hits for Query ID 'Q7Z2W4'.\n",
      "FASTA file 'Q12965.fasta' created with hits for Query ID 'Q12965'.\n",
      "FASTA file 'P10645.fasta' created with hits for Query ID 'P10645'.\n",
      "FASTA file 'P01966.fasta' created with hits for Query ID 'P01966'.\n",
      "FASTA file 'Q14765.fasta' created with hits for Query ID 'Q14765'.\n",
      "FASTA file 'O00910.fasta' created with hits for Query ID 'O00910'.\n",
      "FASTA files created for each Query ID.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over filtered_queries and write hits to separate FASTA files\n",
    "for query, count in filtered_queries:\n",
    "    # Extract the Query ID from the query string between two \"|\"\n",
    "    query_id = query.split(\"|\")[1].strip()\n",
    "    # Define the file path using the Query ID\n",
    "    file_path = f\"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/{query_id}.fasta\"\n",
    "    # Write hits to the FASTA file\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(f\">{query_id}\\n\")  \n",
    "        # Write hits to the file\n",
    "        for hit_number in range(count):\n",
    "            file.write(f\"Hit_{hit_number+1}\\n\")  \n",
    "    print(f\"FASTA file '{query_id}.fasta' created with hits for Query ID '{query_id}'.\")\n",
    "\n",
    "print(\"FASTA files created for each Query ID.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of FASTA files in '/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides': 38\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Directory containing FASTA files\n",
    "fasta_directory = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides\"\n",
    "\n",
    "# Define the pattern to match FASTA files (assuming they end with .fasta or .fa)\n",
    "fasta_pattern = os.path.join(fasta_directory, \"*.fasta\")  # Change \"*.fasta\" to \"*.fa\" if needed\n",
    "\n",
    "# Count the number of matching files\n",
    "fasta_file_count = len(glob.glob(fasta_pattern))\n",
    "\n",
    "# Print the count of FASTA files\n",
    "print(f\"Number of FASTA files in '{fasta_directory}': {fasta_file_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Separate queries into separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries have been split into separate files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def extract_accession_number(header):\n",
    "    \"\"\"Extract accession number from the header.\"\"\"\n",
    "    return header.split(\"|\")[1]\n",
    "\n",
    "def split_queries(input_file):\n",
    "    \"\"\"Split queries into separate files.\"\"\"\n",
    "    # Create a directory to store the output files\n",
    "    output_directory = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Open the input file\n",
    "    with open(input_file, \"r\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Split content into queries\n",
    "    queries = content.split(\">\")[1:]\n",
    "\n",
    "    # Write each query to a separate file\n",
    "    for query in queries:\n",
    "        # Extract header and sequence\n",
    "        header, sequence = query.split(\"\\n\", 1)\n",
    "        accession_number = extract_accession_number(header)\n",
    "        \n",
    "        # Write query to a separate file\n",
    "        output_filename = os.path.join(output_directory, f\"{accession_number}.fasta\")\n",
    "        with open(output_filename, \"w\") as f:\n",
    "            f.write(f\">{header}\\n{sequence}\")\n",
    "\n",
    "    print(\"Queries have been split into separate files.\")\n",
    "\n",
    "# Input \n",
    "split_queries(\"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_viral_seqs.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Add the 2 subunits for P42574"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fasta(header, sequence, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(\">\" + header + \"\\n\")\n",
    "        f.write(sequence + \"\\n\")\n",
    "\n",
    "# Given text\n",
    "text1 = '''>sp|P42574|29-175\n",
    "SGISLDNSYKMDYPEMGLCIIINNKNFHKSTGMTSRSGTDVDAANLRETFRNLKYEVRNK\n",
    "NDLTREEIVELMRDVSKEDHSKRSSFVCVLLSHGEEGIIFGTNGPVDLKKITNFFRGDRC\n",
    "RSLTGKPKLFIIQACRGTELDCGIETD'''\n",
    "\n",
    "text2 = '''>sp|P42574|176-277\n",
    "SGVDDDMACHKIPVEADFLYAYSTAPGYYSWRNSKDGSWFIQSLCAMLKQYADKLEFMHI\n",
    "LTRVNRKVATEFESFSFDATFHAKKQIPCIVSMLTKELYFYH'''\n",
    "\n",
    "# Extracting header and sequence from text\n",
    "header1, sequence1 = text1.split('\\n', 1)\n",
    "header2, sequence2 = text2.split('\\n', 1)\n",
    "\n",
    "# Writing to files\n",
    "write_fasta(header1[1:], sequence1, \"P42574_1.fasta\")\n",
    "write_fasta(header2[1:], sequence2, \"P42574_2.fasta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm P42574.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Run blast on processed files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Q07812.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/Q07812_blast_results.txt\n",
      "Skipping Q8NFJ6.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/Q8NFJ6_blast_results.txt\n",
      "Skipping Q15303.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/Q15303_blast_results.txt\n",
      "Skipping O00910.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/O00910_blast_results.txt\n",
      "Skipping P38398.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P38398_blast_results.txt\n",
      "Skipping P46663.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P46663_blast_results.txt\n",
      "Skipping P32247.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P32247_blast_results.txt\n",
      "Skipping P60484.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P60484_blast_results.txt\n",
      "Skipping P10645.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P10645_blast_results.txt\n",
      "Skipping P01344.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P01344_blast_results.txt\n",
      "Skipping O14879.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/O14879_blast_results.txt\n",
      "Skipping O95390.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/O95390_blast_results.txt\n",
      "Skipping Q00534.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/Q00534_blast_results.txt\n",
      "Skipping P20592.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P20592_blast_results.txt\n",
      "Skipping P06493.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P06493_blast_results.txt\n",
      "Skipping P24941.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P24941_blast_results.txt\n",
      "Skipping P55265.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P55265_blast_results.txt\n",
      "Skipping P06396.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P06396_blast_results.txt\n",
      "Skipping P01966.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P01966_blast_results.txt\n",
      "Skipping P47898.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P47898_blast_results.txt\n",
      "Skipping P42574_2.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P42574_2_blast_results.txt\n",
      "Skipping P11802.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P11802_blast_results.txt\n",
      "Skipping Q14790.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/Q14790_blast_results.txt\n",
      "Skipping Q00535.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/Q00535_blast_results.txt\n",
      "Skipping P43490.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P43490_blast_results.txt\n",
      "Skipping Q00526.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/Q00526_blast_results.txt\n",
      "Skipping Q16611.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/Q16611_blast_results.txt\n",
      "Skipping Q7Z2W4.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/Q7Z2W4_blast_results.txt\n",
      "Skipping P19525.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P19525_blast_results.txt\n",
      "Skipping O43808.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/O43808_blast_results.txt\n",
      "Skipping P01019.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P01019_blast_results.txt\n",
      "Skipping Q14765.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/Q14765_blast_results.txt\n",
      "Skipping Q02388.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/Q02388_blast_results.txt\n",
      "Skipping O95786.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/O95786_blast_results.txt\n",
      "Skipping Q12965.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/Q12965_blast_results.txt\n",
      "Skipping P42574_1.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P42574_1_blast_results.txt\n",
      "Skipping P20591.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P20591_blast_results.txt\n",
      "Skipping P19838.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P19838_blast_results.txt\n",
      "Skipping P25445.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P25445_blast_results.txt\n",
      "Skipping P55210.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/P55210_blast_results.txt\n",
      "Total execution time: 0 minutes 1.24 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# fasta inputs\n",
    "fasta_directory = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq\"\n",
    "\n",
    "# Directory for results\n",
    "blast_output_directory = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/\"\n",
    "\n",
    "# BLAST parameters\n",
    "blast_db = \"genomic/Viruses/Viral_Protein_Sequences\"\n",
    "evalue = \"0.06\"\n",
    "outfmt = \"5\"  \n",
    "blast_remote = \"-remote\"  \n",
    "\n",
    "# 5min interval for logbook\n",
    "print_interval = 300 \n",
    "\n",
    "# Set initial time for printing logbook\n",
    "next_print_time = start_time + print_interval\n",
    "\n",
    "# Select fasta's in directory\n",
    "for filename in os.listdir(fasta_directory):\n",
    "    if filename.endswith(\".fasta\"):  \n",
    "        fasta_file = os.path.join(fasta_directory, filename)\n",
    "        output_file = os.path.splitext(filename)[0] + \"_blast_results.txt\"\n",
    "        output_path = os.path.join(blast_output_directory, output_file)\n",
    "        \n",
    "        # Check if output file already exists\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"Skipping {filename} - Output file already exists: {output_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Record start time for each file\n",
    "        file_start_time = time.time()\n",
    "        \n",
    "        # BLAST command \n",
    "        command = f\"blastp {blast_remote} -query {fasta_file} -db {blast_db} -evalue {evalue} -outfmt {outfmt} -out {output_path}\"\n",
    "        \n",
    "        # Execute BLAST command\n",
    "        os.system(command)\n",
    "        \n",
    "        # Calculate time taken for BLAST search on current file\n",
    "        file_duration = time.time() - file_start_time\n",
    "        \n",
    "        #  to print logbook\n",
    "        if time.time() >= next_print_time:\n",
    "            # Print logbook message\n",
    "            print(f\"BLAST for {filename} started at {time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(file_start_time))} and completed in {file_duration//60:.0f} minutes {file_duration%60:.2f} seconds. Results saved in {output_path}\")\n",
    "            \n",
    "            # Update next print time\n",
    "            next_print_time += print_interval\n",
    "        \n",
    "# Calculate total run time\n",
    "total_duration = time.time() - start_time\n",
    "\n",
    "# Print total execution time\n",
    "print(f\"Total execution time: {total_duration//60:.0f} minutes {total_duration%60:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Combine Viral Blast Results into a csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring file: P42574_2_blast_results.txt - No hits found\n",
      "Ignoring file: Q14790_blast_results.txt - No hits found\n",
      "Included files: 38\n",
      "Ignored files: 2\n",
      "CSV generation complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Path to the directory containing blast output files\n",
    "results_directory = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast\"\n",
    "\n",
    "# Output CSV file path\n",
    "output_csv_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/processed_blasts.csv\"\n",
    "\n",
    "# Initialize count variables\n",
    "included_files_count = 0\n",
    "ignored_files_count = 0\n",
    "\n",
    "# Write CSV header\n",
    "with open(output_csv_path, \"w\", newline=\"\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow([\"File Name\", \"Accession ID\", \"Hit Number\", \"Hit ID\", \"Hit Length\", \"Species\", \"Query Definition\", \"Hsp Bit Score\", \"Hsp Score\", \"Hsp E-value\", \"Identity\", \"Positives\", \"Gaps\", \"Hsp Qseq\", \"Hsp Hseq\", \"Database\"])\n",
    "\n",
    "# Function to extract text between XML tags\n",
    "def extract_text(pattern, text):\n",
    "    match = re.search(pattern, text)\n",
    "    return match.group(1) if match else \"\"\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for filename in os.listdir(results_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        results_file_path = os.path.join(results_directory, filename)\n",
    "        \n",
    "        # Open and read the results\n",
    "        with open(results_file_path, \"r\") as results_file:\n",
    "            results_text = results_file.read()\n",
    "\n",
    "            # Check if \"0 hits found\" or \"No hits found\" is present\n",
    "            if \"0 hits found\" in results_text or \"No hits found\" in results_text:\n",
    "                print(f\"Ignoring file: {filename} - No hits found\")\n",
    "                ignored_files_count += 1\n",
    "            else:\n",
    "                # Check if the file contains expected XML structure\n",
    "                if \"<Hit>\" not in results_text:\n",
    "                    print(f\"Ignoring file: {filename} - Unexpected format\")\n",
    "                    ignored_files_count += 1\n",
    "                else:\n",
    "                    # Increment included files count\n",
    "                    included_files_count += 1\n",
    "\n",
    "                    # Use file names to get Accession ID's\n",
    "                    accession_id = os.path.splitext(filename)[0].replace(\"_blast_results\", \"\")\n",
    "\n",
    "                    # Split the text into individual hits\n",
    "                    hits = re.split(r'</Hit>', results_text)\n",
    "                    hit_number = 1\n",
    "\n",
    "                    # Iterate over each hit\n",
    "                    for hit in hits:\n",
    "                        if '<Hit>' in hit:\n",
    "                            hit_id = extract_text(r'<Hit_id>.*?\\|(.*?)\\|</Hit_id>', hit)\n",
    "                            hit_length = extract_text(r'<Hit_len>(\\d+)</Hit_len>', hit)\n",
    "                            species = extract_text(r'\\[(.*?)\\]', hit)\n",
    "                            query_definition = extract_text(r'<Hit_def>(.*?)<', hit)\n",
    "\n",
    "                            hsps = re.findall(r'<Hsp>.*?</Hsp>', hit, re.DOTALL)\n",
    "                            for hsp in hsps:\n",
    "                                hsp_bit_score = extract_text(r'<Hsp_bit-score>(.*?)</Hsp_bit-score>', hsp)\n",
    "                                hsp_score = extract_text(r'<Hsp_score>(.*?)</Hsp_score>', hsp)\n",
    "                                hsp_evalue = extract_text(r'<Hsp_evalue>(.*?)</Hsp_evalue>', hsp)\n",
    "                                hsp_identity = extract_text(r'<Hsp_identity>(.*?)</Hsp_identity>', hsp)\n",
    "                                hsp_positive = extract_text(r'<Hsp_positive>(.*?)</Hsp_positive>', hsp)\n",
    "                                hsp_gaps = extract_text(r'<Hsp_gaps>(.*?)</Hsp_gaps>', hsp)\n",
    "                                hsp_qseq = extract_text(r'<Hsp_qseq>(.*?)</Hsp_qseq>', hsp)\n",
    "                                hsp_hseq = extract_text(r'<Hsp_hseq>(.*?)</Hsp_hseq>', hsp)\n",
    "\n",
    "                                # Write to CSV\n",
    "                                with open(output_csv_path, \"a\", newline=\"\") as csvfile:\n",
    "                                    csv_writer = csv.writer(csvfile)\n",
    "                                    csv_writer.writerow([filename, accession_id, f\"Hit {hit_number}\", hit_id, hit_length, species, query_definition, hsp_bit_score, hsp_score, hsp_evalue, hsp_identity, hsp_positive, hsp_gaps, hsp_qseq, hsp_hseq, \"Genomic Viruses\"])\n",
    "                                \n",
    "                                hit_number += 1\n",
    "\n",
    "# Print count of included and ignored files\n",
    "print(f\"Included files: {included_files_count}\")\n",
    "print(f\"Ignored files: {ignored_files_count}\")\n",
    "print(\"CSV generation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P42574_1\n",
      "P19838\n",
      "P43490\n",
      "P55265\n",
      "O00910\n",
      "O95786\n",
      "Q8NFJ6\n",
      "P20591\n",
      "O43808\n",
      "P10645\n",
      "P11802\n",
      "P32247\n",
      "Q00535\n",
      "P38398\n",
      "P19525\n",
      "P01344\n",
      "Q15303\n",
      "P20592\n",
      "Q7Z2W4\n",
      "P46663\n",
      "O14879\n",
      "Q00526\n",
      "P60484\n",
      "P01019\n",
      "P55210\n",
      "P24941\n",
      "P25445\n",
      "Q16611\n",
      "P47898\n",
      "Q07812\n",
      "Q14765\n",
      "P06396\n",
      "P01966\n",
      "P06493\n",
      "Q02388\n",
      "Q12965\n",
      "O95390\n",
      "Q00534\n",
      "Total unique Accession IDs: 38\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Path to the output CSV file\n",
    "output_csv_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/processed_blasts.csv\"\n",
    "\n",
    "# Initialize a set to store unique Accession IDs\n",
    "unique_accession_ids = set()\n",
    "\n",
    "# Read the CSV file and extract Accession IDs\n",
    "with open(output_csv_path, \"r\") as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    next(csv_reader)  # Skip the header\n",
    "    for row in csv_reader:\n",
    "        accession_id = row[1]  # Assuming the Accession ID is in the second column\n",
    "        unique_accession_ids.add(accession_id)\n",
    "\n",
    "# Print all unique Accession IDs\n",
    "for accession_id in unique_accession_ids:\n",
    "    print(accession_id)\n",
    "\n",
    "# Print the count of unique Accession IDs\n",
    "print(f\"Total unique Accession IDs: {len(unique_accession_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Display number of hits per Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked list of queries based on the number of hits:\n",
      "1. Query ID: Q02388_blast_results.txt,Q02388,Hit, Hits: 7857\n",
      "2. Query ID: P19838_blast_results.txt,P19838,Hit, Hits: 862\n",
      "3. Query ID: Q15303_blast_results.txt,Q15303,Hit, Hits: 674\n",
      "4. Query ID: P19525_blast_results.txt,P19525,Hit, Hits: 664\n",
      "5. Query ID: Q00534_blast_results.txt,Q00534,Hit, Hits: 596\n",
      "6. Query ID: P06493_blast_results.txt,P06493,Hit, Hits: 577\n",
      "7. Query ID: Q00526_blast_results.txt,Q00526,Hit, Hits: 572\n",
      "8. Query ID: P24941_blast_results.txt,P24941,Hit, Hits: 527\n",
      "9. Query ID: P11802_blast_results.txt,P11802,Hit, Hits: 519\n",
      "10. Query ID: Q00535_blast_results.txt,Q00535,Hit, Hits: 519\n",
      "11. Query ID: Q8NFJ6_blast_results.txt,Q8NFJ6,Hit, Hits: 500\n",
      "12. Query ID: P32247_blast_results.txt,P32247,Hit, Hits: 500\n",
      "13. Query ID: P43490_blast_results.txt,P43490,Hit, Hits: 500\n",
      "14. Query ID: P46663_blast_results.txt,P46663,Hit, Hits: 500\n",
      "15. Query ID: P01019_blast_results.txt,P01019,Hit, Hits: 500\n",
      "16. Query ID: P55265_blast_results.txt,P55265,Hit, Hits: 417\n",
      "17. Query ID: P47898_blast_results.txt,P47898,Hit, Hits: 75\n",
      "18. Query ID: P25445_blast_results.txt,P25445,Hit, Hits: 59\n",
      "19. Query ID: P38398_blast_results.txt,P38398,Hit, Hits: 33\n",
      "20. Query ID: O95786_blast_results.txt,O95786,Hit, Hits: 26\n",
      "21. Query ID: P20591_blast_results.txt,P20591,Hit, Hits: 25\n",
      "22. Query ID: P20592_blast_results.txt,P20592,Hit, Hits: 24\n",
      "23. Query ID: O43808_blast_results.txt,O43808,Hit, Hits: 15\n",
      "24. Query ID: P55210_blast_results.txt,P55210,Hit, Hits: 10\n",
      "25. Query ID: Q07812_blast_results.txt,Q07812,Hit, Hits: 9\n",
      "26. Query ID: P01344_blast_results.txt,P01344,Hit, Hits: 8\n",
      "27. Query ID: O95390_blast_results.txt,O95390,Hit, Hits: 7\n",
      "28. Query ID: P42574_1_blast_results.txt,P42574_1,Hit, Hits: 6\n",
      "29. Query ID: O14879_blast_results.txt,O14879,Hit, Hits: 5\n",
      "30. Query ID: Q16611_blast_results.txt,Q16611,Hit, Hits: 4\n",
      "31. Query ID: P06396_blast_results.txt,P06396,Hit, Hits: 4\n",
      "32. Query ID: P60484_blast_results.txt,P60484,Hit, Hits: 4\n",
      "33. Query ID: Q7Z2W4_blast_results.txt,Q7Z2W4,Hit, Hits: 2\n",
      "34. Query ID: Q12965_blast_results.txt,Q12965,Hit, Hits: 2\n",
      "35. Query ID: P10645_blast_results.txt,P10645,Hit, Hits: 2\n",
      "36. Query ID: File, Hits: 1\n",
      "37. Query ID: P01966_blast_results.txt,P01966,Hit, Hits: 1\n",
      "38. Query ID: Q14765_blast_results.txt,Q14765,Hit, Hits: 1\n",
      "39. Query ID: O00910_blast_results.txt,O00910,Hit, Hits: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Path to the master file\n",
    "master_filepath = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/processed_blasts.csv\"\n",
    "\n",
    "# Read the contents of the master file\n",
    "with open(master_filepath, \"r\") as master_file:\n",
    "    # Read all lines from the file\n",
    "    lines = master_file.readlines()\n",
    "\n",
    "# Extract query sequence IDs from each line\n",
    "query_ids = [line.split()[0] for line in lines if line.strip()]\n",
    "\n",
    "# Count the occurrences of each query ID\n",
    "query_counts = Counter(query_ids)\n",
    "\n",
    "# Sort the queries based on the number of hits (descending order)\n",
    "ranked_queries = sorted(query_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the ranked list of queries\n",
    "print(\"Ranked list of queries based on the number of hits:\")\n",
    "for rank, (query, count) in enumerate(ranked_queries, start=1):\n",
    "    print(f\"{rank}. Query ID: {query}, Hits: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "protein_names_df = pd.read_csv(\"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/protein_names.csv\")\n",
    "processed_blast_df = pd.read_csv(\"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/processed_blasts.csv\")\n",
    "\n",
    "# Perform a left merge to add the protein names to the processed_blast_df\n",
    "merged_df = pd.merge(processed_blast_df, protein_names_df, on=\"Accession ID\", how=\"left\")\n",
    "\n",
    "# Reorder columns to display the Protein Name column at the start\n",
    "merged_df = merged_df[['Protein Names'] + [col for col in merged_df.columns if col != 'Protein Name']]\n",
    "\n",
    "# Write the merged dataframe back to a CSV file with the new name\n",
    "merged_df.to_csv(\"viral_processed_blast.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "protein_names_df = pd.read_csv(\"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/protein_names.csv\")\n",
    "processed_blast_df = pd.read_csv(\"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/processed_blasts.csv\")\n",
    "\n",
    "# Perform a left merge to add the protein names to the processed_blast_df\n",
    "merged_df = pd.merge(processed_blast_df, protein_names_df[['Accession ID', 'Protein Names']], on=\"Accession ID\", how=\"left\")\n",
    "\n",
    "# Reorder columns\n",
    "columns = ['Protein Names'] + [col for col in processed_blast_df.columns]\n",
    "merged_df = merged_df[columns]\n",
    "\n",
    "# Write the merged dataframe back to a CSV file\n",
    "merged_df.to_csv(\"viral_processed_blast.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0  Upload Peptides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Move results of txt file containing viral peptide information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir viral_peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_processed_peptides.txt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mv /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_processed_peptides.txt /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Generate multiple fasta files for each accession ID and populate with the matching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q15303_1\n",
      "P01019_1\n",
      "P01019_2\n",
      "P01019_3\n",
      "P01019_4\n",
      "P01019_5\n",
      "P01019_6\n",
      "P01019_7\n",
      "P01019_8\n",
      "P01344_1\n",
      "P01966_1\n",
      "P10645_1\n",
      "P10645_2\n",
      "P10645_3\n",
      "P10645_4\n",
      "P10645_5\n",
      "P10645_6\n",
      "P10645_7\n",
      "P10645_8\n",
      "P10645_9\n",
      "P10645_10\n",
      "P10645_11\n",
      "P10645_12\n",
      "P10645_13\n",
      "P10645_14\n",
      "P01241_1\n",
      "P01570_1\n",
      "P02533_1\n",
      "P10082_1\n",
      "P10082_2\n",
      "P42574_1\n",
      "P42574_2\n",
      "P52630_1\n",
      "Q03252_1\n",
      "Q10589_1\n",
      "Q96BS2_1\n",
      "Q99988_1\n",
      "P15502_1\n"
     ]
    }
   ],
   "source": [
    "#Command to create a fasta file for each Accession ID. \n",
    "# The command denotes queries of different amino acid sequences under the same accession ID using \"_{number}\" sequentially. \n",
    "# Command also parses out fasta information out and adds it to the newly made fasta's. \n",
    "\n",
    "file_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/comp_viral_processed_peptides.txt\"\n",
    "output_dir = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides\"\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def process_fasta(file_path, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    accession_count = {}\n",
    "    sequence_data = {}\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    current_header = None\n",
    "    current_sequence = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.startswith('>'):\n",
    "            if current_header:\n",
    "                accession_id = current_header.split('|')[1]\n",
    "                accession_id_with_suffix = f\"{accession_id}_{accession_count[accession_id]}\"\n",
    "                sequence_data[accession_id_with_suffix] = (current_header, ''.join(current_sequence))\n",
    "            \n",
    "            parts = line.split('|')\n",
    "            accession_id = parts[1]\n",
    "            \n",
    "            if accession_id not in accession_count:\n",
    "                accession_count[accession_id] = 1\n",
    "            else:\n",
    "                accession_count[accession_id] += 1\n",
    "            \n",
    "            current_header = line.strip()\n",
    "            current_sequence = []\n",
    "        else:\n",
    "            current_sequence.append(line.strip())\n",
    "    \n",
    "    if current_header:\n",
    "        accession_id = current_header.split('|')[1]\n",
    "        accession_id_with_suffix = f\"{accession_id}_{accession_count[accession_id]}\"\n",
    "        sequence_data[accession_id_with_suffix] = (current_header, ''.join(current_sequence))\n",
    "    \n",
    "    for accession_id_with_suffix, (header, sequence) in sequence_data.items():\n",
    "        print(accession_id_with_suffix)\n",
    "        fasta_filename = os.path.join(output_dir, f\"{accession_id_with_suffix}.fasta\")\n",
    "        with open(fasta_filename, 'w') as fasta_file:\n",
    "            fasta_file.write(f\"{header}\\n{sequence}\\n\")\n",
    "            \n",
    "process_fasta(file_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Run blast on viral peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir  /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping P01570_1.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01570_1_blast_results.xml\n",
      "Skipping P10645_3.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_3_blast_results.xml\n",
      "Skipping P10645_1.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_1_blast_results.xml\n",
      "Skipping Q15303_1.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/Q15303_1_blast_results.xml\n",
      "Skipping P01019_3.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_3_blast_results.xml\n",
      "Skipping P01019_6.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_6_blast_results.xml\n",
      "Skipping P10645_13.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_13_blast_results.xml\n",
      "Skipping P15502_1.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P15502_1_blast_results.xml\n",
      "Skipping P10645_4.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_4_blast_results.xml\n",
      "BLAST for P52630_1.fasta completed successfully.\n",
      "Skipping P10645_9.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_9_blast_results.xml\n",
      "BLAST for Q96BS2_1.fasta completed successfully.\n",
      "Skipping P10645_14.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_14_blast_results.xml\n",
      "Skipping Q10589_1.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/Q10589_1_blast_results.xml\n",
      "Skipping P10082_1.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10082_1_blast_results.xml\n",
      "Skipping P10645_2.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_2_blast_results.xml\n",
      "Skipping P42574_2.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P42574_2_blast_results.xml\n",
      "Skipping P01019_5.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_5_blast_results.xml\n",
      "Skipping P01966_1.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01966_1_blast_results.xml\n",
      "Skipping P02533_1.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P02533_1_blast_results.xml\n",
      "Skipping P10645_6.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_6_blast_results.xml\n",
      "Skipping Q99988_1.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/Q99988_1_blast_results.xml\n",
      "Skipping P01019_7.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_7_blast_results.xml\n",
      "Skipping P01019_8.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_8_blast_results.xml\n",
      "Skipping P10645_5.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_5_blast_results.xml\n",
      "Skipping P01019_1.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_1_blast_results.xml\n",
      "Skipping P01344_1.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01344_1_blast_results.xml\n",
      "Skipping P01019_4.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_4_blast_results.xml\n",
      "Skipping P10645_7.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_7_blast_results.xml\n",
      "Skipping P10645_8.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_8_blast_results.xml\n",
      "Skipping Q03252_1.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/Q03252_1_blast_results.xml\n",
      "Skipping P01241_1.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01241_1_blast_results.xml\n",
      "Skipping P10082_2.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10082_2_blast_results.xml\n",
      "Skipping P42574_1.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P42574_1_blast_results.xml\n",
      "Skipping P10645_12.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_12_blast_results.xml\n",
      "Skipping P10645_11.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_11_blast_results.xml\n",
      "Skipping P01019_2.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_2_blast_results.xml\n",
      "Skipping P10645_10.fasta - Output file already exists: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_10_blast_results.xml\n",
      "Total execution time: 1 minutes 39.82 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# fasta inputs\n",
    "fasta_directory = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides\"\n",
    "\n",
    "# Directory for results\n",
    "blast_output_directory = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides\"\n",
    "\n",
    "# BLAST parameters\n",
    "blast_db = \"genomic/Viruses/Viral_Protein_Sequences\"\n",
    "evalue = \"0.06\" \n",
    "outfmt = \"5\"  \n",
    "blast_remote = \"-remote\"  \n",
    "\n",
    "# 5min interval for logbook\n",
    "print_interval = 300 \n",
    "\n",
    "# Set initial time for printing logbook\n",
    "next_print_time = start_time + print_interval\n",
    "\n",
    "# Select fasta's in directory\n",
    "for filename in os.listdir(fasta_directory):\n",
    "    if filename.endswith(\".fasta\"):  \n",
    "        fasta_file = os.path.join(fasta_directory, filename)\n",
    "        output_file = os.path.splitext(filename)[0] + \"_blast_results.xml\"  \n",
    "        output_path = os.path.join(blast_output_directory, output_file)\n",
    "        \n",
    "        # Check if output file already exists\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"Skipping {filename} - Output file already exists: {output_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Record start time for each file\n",
    "        file_start_time = time.time()\n",
    "        \n",
    "        # BLAST command \n",
    "        command = f\"blastp {blast_remote} -query {fasta_file} -db {blast_db} -evalue {evalue} -outfmt {outfmt} -out {output_path}\"\n",
    "        \n",
    "        # Execute BLAST command\n",
    "        try:\n",
    "            result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            if result.returncode == 0:\n",
    "                print(f\"BLAST for {filename} completed successfully.\")\n",
    "            else:\n",
    "                print(f\"BLAST for {filename} failed with return code {result.returncode}.\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"BLAST for {filename} failed with error: {e.stderr.decode()}\")\n",
    "        \n",
    "        # Calculate time taken for BLAST search on current file\n",
    "        file_duration = time.time() - file_start_time\n",
    "        \n",
    "        # Print logbook message\n",
    "        if time.time() >= next_print_time:\n",
    "            print(f\"BLAST for {filename} started at {time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(file_start_time))} and completed in {file_duration//60:.0f} minutes {file_duration%60:.2f} seconds. Results saved in {output_path}\")\n",
    "            next_print_time += print_interval\n",
    "        \n",
    "# Calculate total run time\n",
    "total_duration = time.time() - start_time\n",
    "\n",
    "# Print total execution time\n",
    "print(f\"Total execution time: {total_duration//60:.0f} minutes {total_duration%60:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Run RefSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process started at: 2024-07-14 20:22:30 Dublin local time\n",
      "Skipping P01570_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01570_1_refseq_blastp_results.txt already exists.\n",
      "Skipping P10645_3.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_3_refseq_blastp_results.txt already exists.\n",
      "Skipping P10645_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_1_refseq_blastp_results.txt already exists.\n",
      "Running blast search for Q15303_1.fasta (Accession ID: sp|Q15303|676-1308)...\n",
      "BLAST command executed successfully for Q15303_1.fasta.\n",
      "Blast search for Q15303_1.fasta completed in 795.71 seconds.\n",
      "Skipping P01019_3.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_3_refseq_blastp_results.txt already exists.\n",
      "Skipping P01019_6.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_6_refseq_blastp_results.txt already exists.\n",
      "Skipping P10645_13.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_13_refseq_blastp_results.txt already exists.\n",
      "Skipping P15502_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P15502_1_refseq_blastp_results.txt already exists.\n",
      "Skipping P10645_4.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_4_refseq_blastp_results.txt already exists.\n",
      "Skipping P52630_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P52630_1_refseq_blastp_results.txt already exists.\n",
      "Skipping P10645_9.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_9_refseq_blastp_results.txt already exists.\n",
      "Skipping Q96BS2_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/Q96BS2_1_refseq_blastp_results.txt already exists.\n",
      "Skipping P10645_14.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_14_refseq_blastp_results.txt already exists.\n",
      "Skipping Q10589_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/Q10589_1_refseq_blastp_results.txt already exists.\n",
      "Skipping P10082_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10082_1_refseq_blastp_results.txt already exists.\n",
      "Skipping P10645_2.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_2_refseq_blastp_results.txt already exists.\n",
      "Skipping P42574_2.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P42574_2_refseq_blastp_results.txt already exists.\n",
      "Skipping P01019_5.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_5_refseq_blastp_results.txt already exists.\n",
      "Skipping P01966_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01966_1_refseq_blastp_results.txt already exists.\n",
      "Skipping P02533_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P02533_1_refseq_blastp_results.txt already exists.\n",
      "Skipping P10645_6.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_6_refseq_blastp_results.txt already exists.\n",
      "Skipping Q99988_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/Q99988_1_refseq_blastp_results.txt already exists.\n",
      "Skipping P01019_7.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_7_refseq_blastp_results.txt already exists.\n",
      "Skipping P01019_8.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_8_refseq_blastp_results.txt already exists.\n",
      "Skipping P10645_5.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_5_refseq_blastp_results.txt already exists.\n",
      "Skipping P01019_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_1_refseq_blastp_results.txt already exists.\n",
      "Skipping P01344_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01344_1_refseq_blastp_results.txt already exists.\n",
      "Skipping P01019_4.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_4_refseq_blastp_results.txt already exists.\n",
      "Skipping P10645_7.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_7_refseq_blastp_results.txt already exists.\n",
      "Skipping P10645_8.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_8_refseq_blastp_results.txt already exists.\n",
      "Skipping Q03252_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/Q03252_1_refseq_blastp_results.txt already exists.\n",
      "Skipping P01241_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01241_1_refseq_blastp_results.txt already exists.\n",
      "Skipping P10082_2.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10082_2_refseq_blastp_results.txt already exists.\n",
      "Skipping P42574_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P42574_1_refseq_blastp_results.txt already exists.\n",
      "Skipping P10645_12.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_12_refseq_blastp_results.txt already exists.\n",
      "Skipping P10645_11.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_11_refseq_blastp_results.txt already exists.\n",
      "Skipping P01019_2.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_2_refseq_blastp_results.txt already exists.\n",
      "Skipping P10645_10.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_10_refseq_blastp_results.txt already exists.\n",
      "Blastp searches completed for all fasta files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "# Function to convert GMT to Dublin local time\n",
    "def gmt_to_dublin(gmt_time):\n",
    "    dublin_offset = timedelta(hours=1)  \n",
    "    return gmt_time + dublin_offset\n",
    "\n",
    "# Paths\n",
    "fasta_directory = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides\"\n",
    "blast_output_directory = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides\"\n",
    "\n",
    "# Create the blast output directory if it doesn't exist\n",
    "os.makedirs(blast_output_directory, exist_ok=True)\n",
    "\n",
    "# Record the start time of the entire process in GMT\n",
    "start_time = datetime.now(timezone.utc)\n",
    "start_time_dublin = gmt_to_dublin(start_time)\n",
    "print(f\"Process started at: {start_time_dublin.strftime('%Y-%m-%d %H:%M:%S')} Dublin local time\")\n",
    "\n",
    "def get_accession_id(fasta_file):\n",
    "    with open(fasta_file, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('>'):\n",
    "                return line.split()[0][1:]  \n",
    "    return None\n",
    "\n",
    "# Iterate through each fasta file in the directory\n",
    "for filename in os.listdir(fasta_directory):\n",
    "    if filename.endswith(\".fasta\"):\n",
    "        fasta_file = os.path.join(fasta_directory, filename)\n",
    "        accession_id = get_accession_id(fasta_file)\n",
    "        if not accession_id:\n",
    "            print(f\"No accession ID found in {filename}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        output_file = os.path.join(blast_output_directory, f\"{filename.split('.')[0]}_refseq_blastp_results.txt\")\n",
    "        \n",
    "        # Skip if output file already exists\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Skipping {filename} as output file {output_file} already exists.\")\n",
    "            continue\n",
    "        \n",
    "        # Construct the blastp command for remote search\n",
    "        blastp_command = [\n",
    "            \"blastp\",\n",
    "            \"-query\", fasta_file,\n",
    "            \"-db\", \"refseq_protein\",\n",
    "            \"-out\", output_file,\n",
    "            \"-evalue\", \"0.06\",\n",
    "            \"-entrez_query\", \"viruses[orgn]\",\n",
    "            \"-remote\"\n",
    "        ]\n",
    "        \n",
    "        # Record start time for this file\n",
    "        file_start_time = time.time()\n",
    "        \n",
    "        # Execute the blastp command\n",
    "        print(f\"Running blast search for {filename} (Accession ID: {accession_id})...\")\n",
    "        try:\n",
    "            result = subprocess.run(blastp_command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            if result.returncode == 0:\n",
    "                print(f\"BLAST command executed successfully for {filename}.\")\n",
    "            else:\n",
    "                print(f\"BLAST command failed for {filename} with return code {result.returncode}.\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"BLAST command failed for {filename} with error: {e.stderr.decode()}\")\n",
    "        \n",
    "        # Calculate elapsed time for this file\n",
    "        file_elapsed_time = time.time() - file_start_time\n",
    "        \n",
    "        # Print filename and elapsed time\n",
    "        print(f\"Blast search for {filename} completed in {file_elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "        # Check if 30 minutes have elapsed since the start of the process\n",
    "        if (datetime.now(timezone.utc) - start_time).seconds >= 1800:\n",
    "            print(f\"Process has been running for 30 minutes. Current time: {gmt_to_dublin(datetime.now(timezone.utc)).strftime('%Y-%m-%d %H:%M:%S')} Dublin local time\")\n",
    "            start_time = datetime.now(timezone.utc)\n",
    "\n",
    "print(\"Blastp searches completed for all fasta files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Run Non-Redundancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process started at: 2024-07-14 20:12:52 Dublin local time\n",
      "Skipping P01570_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01570_1_nr_blastp_results.txt already exists.\n",
      "Skipping P10645_3.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_3_nr_blastp_results.txt already exists.\n",
      "Skipping P10645_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_1_nr_blastp_results.txt already exists.\n",
      "Running blast search for Q15303_1.fasta (Accession ID: sp|Q15303|676-1308)...\n",
      "BLAST command executed successfully for Q15303_1.fasta.\n",
      "Blast search for Q15303_1.fasta completed in 197.99 seconds.\n",
      "Skipping P01019_3.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_3_nr_blastp_results.txt already exists.\n",
      "Skipping P01019_6.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_6_nr_blastp_results.txt already exists.\n",
      "Skipping P10645_13.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_13_nr_blastp_results.txt already exists.\n",
      "Skipping P15502_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P15502_1_nr_blastp_results.txt already exists.\n",
      "Skipping P10645_4.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_4_nr_blastp_results.txt already exists.\n",
      "Skipping P52630_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P52630_1_nr_blastp_results.txt already exists.\n",
      "Skipping P10645_9.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_9_nr_blastp_results.txt already exists.\n",
      "Skipping Q96BS2_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/Q96BS2_1_nr_blastp_results.txt already exists.\n",
      "Skipping P10645_14.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_14_nr_blastp_results.txt already exists.\n",
      "Skipping Q10589_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/Q10589_1_nr_blastp_results.txt already exists.\n",
      "Skipping P10082_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10082_1_nr_blastp_results.txt already exists.\n",
      "Skipping P10645_2.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_2_nr_blastp_results.txt already exists.\n",
      "Skipping P42574_2.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P42574_2_nr_blastp_results.txt already exists.\n",
      "Skipping P01019_5.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_5_nr_blastp_results.txt already exists.\n",
      "Skipping P01966_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01966_1_nr_blastp_results.txt already exists.\n",
      "Skipping P02533_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P02533_1_nr_blastp_results.txt already exists.\n",
      "Skipping P10645_6.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_6_nr_blastp_results.txt already exists.\n",
      "Running blast search for Q99988_1.fasta (Accession ID: sp|Q99988|195-308)...\n",
      "BLAST command executed successfully for Q99988_1.fasta.\n",
      "Blast search for Q99988_1.fasta completed in 99.70 seconds.\n",
      "Skipping P01019_7.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_7_nr_blastp_results.txt already exists.\n",
      "Skipping P01019_8.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_8_nr_blastp_results.txt already exists.\n",
      "Skipping P10645_5.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_5_nr_blastp_results.txt already exists.\n",
      "Skipping P01019_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_1_nr_blastp_results.txt already exists.\n",
      "Skipping P01344_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01344_1_nr_blastp_results.txt already exists.\n",
      "Skipping P01019_4.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_4_nr_blastp_results.txt already exists.\n",
      "Skipping P10645_7.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_7_nr_blastp_results.txt already exists.\n",
      "Skipping P10645_8.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_8_nr_blastp_results.txt already exists.\n",
      "Running blast search for Q03252_1.fasta (Accession ID: sp|Q03252|1-617)...\n",
      "BLAST command executed successfully for Q03252_1.fasta.\n",
      "Blast search for Q03252_1.fasta completed in 135.51 seconds.\n",
      "Skipping P01241_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01241_1_nr_blastp_results.txt already exists.\n",
      "Skipping P10082_2.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10082_2_nr_blastp_results.txt already exists.\n",
      "Skipping P42574_1.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P42574_1_nr_blastp_results.txt already exists.\n",
      "Skipping P10645_12.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_12_nr_blastp_results.txt already exists.\n",
      "Skipping P10645_11.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_11_nr_blastp_results.txt already exists.\n",
      "Skipping P01019_2.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P01019_2_nr_blastp_results.txt already exists.\n",
      "Skipping P10645_10.fasta as output file /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/P10645_10_nr_blastp_results.txt already exists.\n",
      "Blastp searches completed for all fasta files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "# Function to convert GMT to Dublin local time\n",
    "def gmt_to_dublin(gmt_time):\n",
    "    dublin_offset = timedelta(hours=1)  \n",
    "    return gmt_time + dublin_offset\n",
    "\n",
    "# Paths\n",
    "fasta_directory = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides\"\n",
    "blast_output_directory = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides\"\n",
    "\n",
    "# Create the blast output directory if it doesn't exist\n",
    "os.makedirs(blast_output_directory, exist_ok=True)\n",
    "\n",
    "# Record the start time of the entire process in GMT\n",
    "start_time = datetime.now(timezone.utc)\n",
    "start_time_dublin = gmt_to_dublin(start_time)\n",
    "print(f\"Process started at: {start_time_dublin.strftime('%Y-%m-%d %H:%M:%S')} Dublin local time\")\n",
    "\n",
    "def get_accession_id(fasta_file):\n",
    "    with open(fasta_file, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('>'):\n",
    "                return line.split()[0][1:]  \n",
    "    return None\n",
    "\n",
    "# Iterate through each fasta file in the directory\n",
    "for filename in os.listdir(fasta_directory):\n",
    "    if filename.endswith(\".fasta\"):\n",
    "        fasta_file = os.path.join(fasta_directory, filename)\n",
    "        accession_id = get_accession_id(fasta_file)\n",
    "        if not accession_id:\n",
    "            print(f\"No accession ID found in {filename}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        output_file = os.path.join(blast_output_directory, f\"{filename.split('.')[0]}_nr_blastp_results.txt\")\n",
    "        \n",
    "        # Skip if output file already exists\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Skipping {filename} as output file {output_file} already exists.\")\n",
    "            continue\n",
    "        \n",
    "        # Construct the blastp command for remote search\n",
    "        blastp_command = [\n",
    "            \"blastp\",\n",
    "            \"-query\", fasta_file,\n",
    "            \"-db\", \"nr\",\n",
    "            \"-out\", output_file,\n",
    "            \"-evalue\", \"0.06\",\n",
    "            \"-entrez_query\", \"viruses[orgn]\",\n",
    "            \"-remote\"\n",
    "        ]\n",
    "        \n",
    "        # Record start time for this file\n",
    "        file_start_time = time.time()\n",
    "        \n",
    "        # Execute the blastp command\n",
    "        print(f\"Running blast search for {filename} (Accession ID: {accession_id})...\")\n",
    "        try:\n",
    "            result = subprocess.run(blastp_command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            if result.returncode == 0:\n",
    "                print(f\"BLAST command executed successfully for {filename}.\")\n",
    "            else:\n",
    "                print(f\"BLAST command failed for {filename} with return code {result.returncode}.\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"BLAST command failed for {filename} with error: {e.stderr.decode()}\")\n",
    "        \n",
    "        # Calculate elapsed time for this file\n",
    "        file_elapsed_time = time.time() - file_start_time\n",
    "        \n",
    "        # Print filename and elapsed time\n",
    "        print(f\"Blast search for {filename} completed in {file_elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "        # Check if 30 minutes have elapsed since the start of the process\n",
    "        if (datetime.now(timezone.utc) - start_time).seconds >= 1800:\n",
    "            print(f\"Process has been running for 30 minutes. Current time: {gmt_to_dublin(datetime.now(timezone.utc)).strftime('%Y-%m-%d %H:%M:%S')} Dublin local time\")\n",
    "            start_time = datetime.now(timezone.utc)\n",
    "\n",
    "print(\"Blastp searches completed for all fasta files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Process Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Process Peptide Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: P10645_7_blast_results.xml\n",
      "Ignoring file: P10645_7_blast_results.xml - No hits found\n",
      "Processing file: P01019_7_blast_results.xml\n",
      "Ignoring file: P01019_7_blast_results.xml - No hits found\n",
      "Processing file: P10645_9_blast_results.xml\n",
      "Ignoring file: P10645_9_blast_results.xml - No hits found\n",
      "Processing file: P10645_11_blast_results.xml\n",
      "Ignoring file: P10645_11_blast_results.xml - No hits found\n",
      "Processing file: P10645_1_blast_results.xml\n",
      "Ignoring file: P10645_1_blast_results.xml - No hits found\n",
      "Processing file: Q96BS2_1_blast_results.xml\n",
      "Ignoring file: Q96BS2_1_blast_results.xml - No hits found\n",
      "Processing file: P42574_1_blast_results.xml\n",
      "Processing file: P10645_8_blast_results.xml\n",
      "Ignoring file: P10645_8_blast_results.xml - No hits found\n",
      "Processing file: P02533_1_blast_results.xml\n",
      "Ignoring file: P02533_1_blast_results.xml - No hits found\n",
      "Processing file: P10645_6_blast_results.xml\n",
      "Ignoring file: P10645_6_blast_results.xml - No hits found\n",
      "Processing file: P10645_12_blast_results.xml\n",
      "Ignoring file: P10645_12_blast_results.xml - No hits found\n",
      "Processing file: P10645_10_blast_results.xml\n",
      "Ignoring file: P10645_10_blast_results.xml - No hits found\n",
      "Processing file: P01019_1_blast_results.xml\n",
      "Ignoring file: P01019_1_blast_results.xml - No hits found\n",
      "Processing file: P52630_1_blast_results.xml\n",
      "Ignoring file: P52630_1_blast_results.xml - No hits found\n",
      "Processing file: P01019_5_blast_results.xml\n",
      "Ignoring file: P01019_5_blast_results.xml - No hits found\n",
      "Processing file: P10645_5_blast_results.xml\n",
      "Ignoring file: P10645_5_blast_results.xml - No hits found\n",
      "Processing file: P15502_1_blast_results.xml\n",
      "Ignoring file: P15502_1_blast_results.xml - No hits found\n",
      "Processing file: P10645_2_blast_results.xml\n",
      "Ignoring file: P10645_2_blast_results.xml - No hits found\n",
      "Processing file: P10645_3_blast_results.xml\n",
      "Ignoring file: P10645_3_blast_results.xml - No hits found\n",
      "Processing file: P10645_13_blast_results.xml\n",
      "Ignoring file: P10645_13_blast_results.xml - No hits found\n",
      "Processing file: Q15303_1_blast_results.xml\n",
      "Processing file: Q03252_1_blast_results.xml\n",
      "Ignoring file: Q03252_1_blast_results.xml - No hits found\n",
      "Processing file: P01966_1_blast_results.xml\n",
      "Ignoring file: P01966_1_blast_results.xml - No hits found\n",
      "Processing file: P01019_8_blast_results.xml\n",
      "Ignoring file: P01019_8_blast_results.xml - No hits found\n",
      "Processing file: P01019_2_blast_results.xml\n",
      "Ignoring file: P01019_2_blast_results.xml - No hits found\n",
      "Processing file: Q10589_1_blast_results.xml\n",
      "Ignoring file: Q10589_1_blast_results.xml - No hits found\n",
      "Processing file: P01241_1_blast_results.xml\n",
      "Ignoring file: P01241_1_blast_results.xml - No hits found\n",
      "Processing file: P01344_1_blast_results.xml\n",
      "Ignoring file: P01344_1_blast_results.xml - No hits found\n",
      "Processing file: P01570_1_blast_results.xml\n",
      "Ignoring file: P01570_1_blast_results.xml - No hits found\n",
      "Processing file: P10645_14_blast_results.xml\n",
      "Ignoring file: P10645_14_blast_results.xml - No hits found\n",
      "Processing file: P10645_4_blast_results.xml\n",
      "Ignoring file: P10645_4_blast_results.xml - No hits found\n",
      "Processing file: P10082_1_blast_results.xml\n",
      "Ignoring file: P10082_1_blast_results.xml - No hits found\n",
      "Processing file: Q99988_1_blast_results.xml\n",
      "Ignoring file: Q99988_1_blast_results.xml - No hits found\n",
      "Processing file: P01019_4_blast_results.xml\n",
      "Ignoring file: P01019_4_blast_results.xml - No hits found\n",
      "Processing file: P42574_2_blast_results.xml\n",
      "Ignoring file: P42574_2_blast_results.xml - No hits found\n",
      "Processing file: P10082_2_blast_results.xml\n",
      "Ignoring file: P10082_2_blast_results.xml - No hits found\n",
      "Processing file: P01019_3_blast_results.xml\n",
      "Ignoring file: P01019_3_blast_results.xml - No hits found\n",
      "Processing file: P01019_6_blast_results.xml\n",
      "Ignoring file: P01019_6_blast_results.xml - No hits found\n",
      "Included files: 2\n",
      "Ignored files: 36\n",
      "CSV generation complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "\n",
    "# Path to the blast outputs in fmt 5\n",
    "results_directory = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides\"\n",
    "\n",
    "# Output CSV file path\n",
    "output_csv_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/viral_peptide_results.csv\"\n",
    "\n",
    "# Initialize count variables\n",
    "included_files_count = 0\n",
    "ignored_files_count = 0\n",
    "\n",
    "# Regular expression pattern to extract text within [ ]\n",
    "pattern = r\"\\[(.*?)\\]\"\n",
    "\n",
    "# Write CSV header with Database column\n",
    "with open(output_csv_path, \"w\", newline=\"\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow([\"File Name\", \"Accession ID\", \"Hit Number\", \"Hit ID\", \"Hit Length\", \"Species\", \"Query Definition\", \"Hsp Bit Score\", \"Hsp Score\", \"Hsp E-value\", \"Identity\", \"Positives\", \"Gaps\", \"Hsp Qseq\", \"Hsp Hseq\", \"Database\"])\n",
    "\n",
    "# Iterate through each blast output\n",
    "for filename in os.listdir(results_directory):\n",
    "    if filename.endswith(\"blast_results.xml\"):\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        results_file_path = os.path.join(results_directory, filename)\n",
    "        \n",
    "        # Determine the database type based on filename\n",
    "        if \"refseq_blastp_results\" in filename:\n",
    "            database_type = \"RefSeq\"\n",
    "        elif \"nr_blastp_results\" in filename:\n",
    "            database_type = \"Non-Redundancy\"\n",
    "        elif \"blast_results.xml\" in filename:\n",
    "            database_type = \"Genomic Viruses\"\n",
    "        else:\n",
    "            database_type = \"Other\"\n",
    "        \n",
    "        # Parse XML file\n",
    "        try:\n",
    "            tree = ET.parse(results_file_path)\n",
    "            root = tree.getroot()\n",
    "        except ET.ParseError as e:\n",
    "            print(f\"Error parsing XML file {filename}: {e}\")\n",
    "            ignored_files_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Check if \"No hits found\"\n",
    "        if root.find(\".//Iteration_message\") is not None and \"No hits found\" in root.find(\".//Iteration_message\").text:\n",
    "            print(f\"Ignoring file: {filename} - No hits found\")\n",
    "            ignored_files_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Increment included files count\n",
    "        included_files_count += 1\n",
    "        \n",
    "        # Accession ID\n",
    "        accession_id = root.find(\".//BlastOutput_query-def\").text\n",
    "        \n",
    "        # Initialize hit number\n",
    "        hit_number = 1\n",
    "        \n",
    "        # Extract hits information\n",
    "        for hit in root.findall(\".//Hit\"):\n",
    "            hit_id = hit.find(\".//Hit_id\").text\n",
    "            hit_length = hit.find(\".//Hit_len\").text\n",
    "            query_definition_text = hit.find(\".//Hit_def\").text \n",
    "            species = \"NA\"\n",
    "            \n",
    "            # Extract species from query definition if it contains [ ]\n",
    "            match = re.search(pattern, query_definition_text)\n",
    "            if match:\n",
    "                species = match.group(1)\n",
    "                query_definition_text = re.sub(pattern, \"\", query_definition_text).strip()\n",
    "            \n",
    "            for hsp in hit.findall(\".//Hsp\"):\n",
    "                # Check if elements exist before accessing text attribute\n",
    "                hsp_bit_score = hsp.find(\".//Hsp_bit-score\")\n",
    "                hsp_score = hsp.find(\".//Hsp_score\")\n",
    "                hsp_evalue = hsp.find(\".//Hsp_evalue\")\n",
    "                hsp_identity = hsp.find(\".//Hsp_identity\")\n",
    "                hsp_positives = hsp.find(\".//Hsp_positive\")\n",
    "                hsp_gaps = hsp.find(\".//Hsp_gaps\")\n",
    "                hsp_qseq = hsp.find(\".//Hsp_qseq\")\n",
    "                hsp_hseq = hsp.find(\".//Hsp_hseq\")\n",
    "                \n",
    "                # Ensure elements are not None before accessing text attribute\n",
    "                hsp_bit_score_text = hsp_bit_score.text if hsp_bit_score is not None else \"\"\n",
    "                hsp_score_text = hsp_score.text if hsp_score is not None else \"\"\n",
    "                hsp_evalue_text = hsp_evalue.text if hsp_evalue is not None else \"\"\n",
    "                hsp_identity_text = hsp_identity.text if hsp_identity is not None else \"\"\n",
    "                hsp_positives_text = hsp_positives.text if hsp_positives is not None else \"\"\n",
    "                hsp_gaps_text = hsp_gaps.text if hsp_gaps is not None else \"\"\n",
    "                hsp_qseq_text = hsp_qseq.text if hsp_qseq is not None else \"\"\n",
    "                hsp_hseq_text = hsp_hseq.text if hsp_hseq is not None else \"\"\n",
    "                \n",
    "                # Write to CSV\n",
    "                with open(output_csv_path, \"a\", newline=\"\") as csvfile:\n",
    "                    csv_writer = csv.writer(csvfile)\n",
    "                    csv_writer.writerow([filename, accession_id, f\"Hit {hit_number}\", hit_id, hit_length, species, query_definition_text, hsp_bit_score_text, hsp_score_text, hsp_evalue_text, hsp_identity_text, hsp_positives_text, hsp_gaps_text, hsp_qseq_text, hsp_hseq_text, database_type])\n",
    "                \n",
    "                hit_number += 1\n",
    "\n",
    "# Print count of included and ignored files\n",
    "print(f\"Included files: {included_files_count}\")\n",
    "print(f\"Ignored files: {ignored_files_count}\")\n",
    "print(\"CSV generation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Process new Non-Redundancies and Refseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Directory containing the BLAST result files\n",
    "input_directory = '/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides'\n",
    "# Output CSV file path\n",
    "output_csv = '/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/NRRS_hits.csv'\n",
    "\n",
    "# Function to determine database type based on filename\n",
    "def determine_database_type(filename):\n",
    "    if \"refseq_blastp_results\" in filename:\n",
    "        return \"RefSeq\"\n",
    "    elif \"nr_blastp_results\" in filename:\n",
    "        return \"Non-Redundancy\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Function to extract accession ID from filename\n",
    "def extract_accession_id(filename):\n",
    "    return filename.split('_')[0]\n",
    "\n",
    "# Function to extract species from hit definition\n",
    "def extract_species(hit_def):\n",
    "    match = re.search(r'\\[(.*?)\\]', hit_def)\n",
    "    return match.group(1) if match else \"Unknown\"\n",
    "\n",
    "# Function to extract query ID from TXT file name\n",
    "def extract_query_id(file_name):\n",
    "    return file_name.replace(\"_blastp_results.txt\", \"\")\n",
    "\n",
    "# Function to extract query definition from TXT hit description\n",
    "def extract_query_definition(line):\n",
    "    if line.startswith(\">\"):\n",
    "        return line.strip()\n",
    "    return None\n",
    "\n",
    "def parse_blast_xml(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Hits and HSPs\n",
    "    hits = []\n",
    "    for hit in root.findall('.//Hit'):\n",
    "        hit_number = hit.find('Hit_num').text\n",
    "        hit_id = hit.find('Hit_id').text\n",
    "        hit_length = hit.find('Hit_len').text\n",
    "        hit_definition = hit.find('Hit_def').text\n",
    "        species = extract_species(hit_definition)\n",
    "\n",
    "        for hsp in hit.findall('Hit_hsps/Hsp'):\n",
    "            bit_score = hsp.find('Hsp_bit-score').text\n",
    "            score = hsp.find('Hsp_score').text\n",
    "            evalue = hsp.find('Hsp_evalue').text\n",
    "            identity = hsp.find('Hsp_identity').text\n",
    "            positives = hsp.find('Hsp_positive').text\n",
    "            gaps = hsp.find('Hsp_gaps').text\n",
    "            query_sequence = hsp.find('Hsp_qseq').text\n",
    "            hit_sequence = hsp.find('Hsp_hseq').text\n",
    "\n",
    "            hits.append([\n",
    "                os.path.basename(file_path), extract_accession_id(os.path.basename(file_path)), f\"Hit {hit_number}\", hit_id, hit_length, species,\n",
    "                hit_definition, bit_score, score, evalue, identity, positives, gaps, query_sequence, hit_sequence,\n",
    "                determine_database_type(os.path.basename(file_path))\n",
    "            ])\n",
    "    \n",
    "    return hits\n",
    "\n",
    "def parse_blast_txt(file_path):\n",
    "    hits_data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        accession_id = extract_query_id(os.path.basename(file_path))\n",
    "        hit_number = 0\n",
    "        hit_id = None\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                hit_number += 1\n",
    "                hit_id = line.split(\">\")[1].split(\" \")[0]\n",
    "                species_split = line.split(\"[\")\n",
    "                if len(species_split) > 1:\n",
    "                    species_id = species_split[1].split(\"]\")[0]\n",
    "                else:\n",
    "                    species_id = None\n",
    "                \n",
    "                query_definition = extract_query_definition(line)\n",
    "                bit_score = \"N/A\"\n",
    "                score = \"N/A\"\n",
    "                evalue = \"N/A\"\n",
    "                identities = \"N/A\"\n",
    "                positives = \"N/A\"\n",
    "                gaps = \"N/A\"\n",
    "                query_sequence = \"N/A\"\n",
    "                subject_sequence = \"N/A\"\n",
    "\n",
    "            elif hit_id and \"Length=\" in line:\n",
    "                hit_length = int(line.split(\"=\")[-1].strip())\n",
    "            elif hit_id and \"Expect =\" in line:\n",
    "                evalue = line.split(\"Expect = \")[1].split(\",\")[0].strip()\n",
    "                score = line.split(\"Score = \")[1].split(\" bits\")[0].strip()\n",
    "            elif hit_id and \"Identities =\" in line:\n",
    "                identities_str = line.split(\"Identities = \")[1].split(\",\")[0].strip()\n",
    "                identities = round(float(identities_str.split(\" \")[0].split(\"/\")[0]) / float(identities_str.split(\" \")[0].split(\"/\")[1]) * 100, 2)\n",
    "                positives_str = line.split(\"Positives = \")[1].split(\",\")[0].strip()\n",
    "                positives = round(float(positives_str.split(\" \")[0].split(\"/\")[0]) / float(positives_str.split(\" \")[0].split(\"/\")[1]) * 100, 2)\n",
    "                gaps_str = line.split(\"Gaps = \")[1].split(\",\")[0].strip()\n",
    "                gaps = round(float(gaps_str.split(\" \")[0].split(\"/\")[0]) / float(gaps_str.split(\" \")[0].split(\"/\")[1]) * 100, 2)\n",
    "            elif hit_id and line.startswith(\"Query=\"):\n",
    "                query_sequence = line.split(\"Query= \")[1].strip()\n",
    "            elif hit_id and line.startswith(\"Sbjct=\"):\n",
    "                subject_sequence = line.split(\"Sbjct= \")[1].strip()\n",
    "                hits_data.append([\n",
    "                    os.path.basename(file_path), accession_id, f\"Hit {hit_number}\", hit_id, hit_length, species_id,\n",
    "                    query_definition, bit_score, score, evalue, identities, positives, gaps, query_sequence, subject_sequence,\n",
    "                    determine_database_type(os.path.basename(file_path))\n",
    "                ])\n",
    "\n",
    "    return hits_data\n",
    "\n",
    "def process_all_blast_files(input_directory, output_csv):\n",
    "    # Collect all BLAST XML and TXT files in the directory\n",
    "    xml_files = glob.glob(os.path.join(input_directory, '*_blast_results.xml'))\n",
    "    txt_files = glob.glob(os.path.join(input_directory, '*_blastp_results.txt'))\n",
    "\n",
    "    # Open CSV file for writing\n",
    "    with open(output_csv, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header row\n",
    "        writer.writerow([\n",
    "            \"File Name\", \"Accession ID\", \"Hit Number\", \"Hit ID\", \"Hit Length\", \"Species\",\n",
    "            \"Query Definition\", \"Hsp Bit Score\", \"Hsp Score\", \"Hsp E-value\",\n",
    "            \"Identity\", \"Positives\", \"Gaps\", \"Hsp Qseq\", \"Hsp Hseq\", \"Database\"\n",
    "        ])\n",
    "        \n",
    "        # Process and write XML files\n",
    "        for xml_file in xml_files:\n",
    "            hits = parse_blast_xml(xml_file)\n",
    "            writer.writerows(hits)\n",
    "        \n",
    "        # Process and write TXT files\n",
    "        for txt_file in txt_files:\n",
    "            hits = parse_blast_txt(txt_file)\n",
    "            writer.writerows(hits)\n",
    "\n",
    "# Process all BLAST XML and TXT files and write the data to the CSV file\n",
    "process_all_blast_files(input_directory, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Concatenate All Processed Viral Blast results to One CSV File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Add Processed seqs and Peptides to one csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been successfully concatenated.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Paths to the input CSV files\n",
    "results_file_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/processed_blasts.csv\"\n",
    "peptides_file_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/comp_viral_peptide_results.csv\"\n",
    "\n",
    "# Path to the output CSV file (same as the first input file to overwrite it)\n",
    "output_csv_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/combined_blasts.csv\"\n",
    "\n",
    "# Function to append the content of one CSV file to another\n",
    "def append_csv(input_file_path, output_file_path, write_header):\n",
    "    with open(input_file_path, 'r', newline='') as input_csvfile:\n",
    "        reader = csv.reader(input_csvfile)\n",
    "        try:\n",
    "            header = next(reader)\n",
    "        except StopIteration:\n",
    "            # The file is empty, skip it\n",
    "            print(f\"Ignoring file: {input_file_path} - File is empty\")\n",
    "            return\n",
    "        \n",
    "        with open(output_file_path, 'a', newline='') as output_csvfile:\n",
    "            writer = csv.writer(output_csvfile)\n",
    "            \n",
    "            if write_header:\n",
    "                writer.writerow(header)\n",
    "            \n",
    "            for row in reader:\n",
    "                writer.writerow(row)\n",
    "\n",
    "# Clear the output file if it exists (to avoid appending to an old file)\n",
    "open(output_csv_path, 'w').close()\n",
    "\n",
    "# Append the contents of the input files to the output file\n",
    "append_csv(results_file_path, output_csv_path, write_header=True)\n",
    "append_csv(peptides_file_path, output_csv_path, write_header=False)\n",
    "\n",
    "print(\"CSV files have been successfully concatenated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Check to confirm a uniform format between csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers for /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/processed_blasts.csv:\n",
      "['File Name', 'Accession ID', 'Hit Number', 'Hit ID', 'Hit Length', 'Species', 'Query Definition', 'Hsp Bit Score', 'Hsp Score', 'Hsp E-value', 'Identity', 'Positives', 'Gaps', 'Hsp Qseq', 'Hsp Hseq', 'Database']\n",
      "\n",
      "Headers for /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/comp_viral_peptide_results.csv:\n",
      "['File Name', 'Accession ID', 'Hit Number', 'Hit ID', 'Hit Length', 'Species', 'Query Definition', 'Hsp Bit Score', 'Hsp Score', 'Hsp E-value', 'Identity', 'Positives', 'Gaps', 'Hsp Qseq', 'Hsp Hseq', 'Database']\n",
      "\n",
      "Headers for /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/NRRS_hits.csv:\n",
      "['File Name', 'Accession ID', 'Hit Number', 'Hit ID', 'Hit Length', 'Species', 'Query Definition', 'Hsp Bit Score', 'Hsp Score', 'Hsp E-value', 'Identity', 'Positives', 'Gaps', 'Hsp Qseq', 'Hsp Hseq', 'Database']\n",
      "\n",
      "Headers for /home/osheakes/Research_Project_MMM/Fasta/TEST/CD47/CD47_data.csv:\n",
      "['File Name', 'Accession ID', 'Hit Number', 'Hit ID', 'Hit Length', 'Species', 'Query Definition', 'Hsp Bit Score', 'Hsp Score', 'Hsp E-value', 'Identity', 'Positives', 'Gaps', 'Hsp Qseq', 'Hsp Hseq', 'Database']\n",
      "\n",
      "Files with identical headers:\n",
      "/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/processed_blasts.csv and /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/comp_viral_peptide_results.csv\n",
      "/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/processed_blasts.csv and /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/NRRS_hits.csv\n",
      "/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/processed_blasts.csv and /home/osheakes/Research_Project_MMM/Fasta/TEST/CD47/CD47_data.csv\n",
      "/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/comp_viral_peptide_results.csv and /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/NRRS_hits.csv\n",
      "/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/comp_viral_peptide_results.csv and /home/osheakes/Research_Project_MMM/Fasta/TEST/CD47/CD47_data.csv\n",
      "/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/NRRS_hits.csv and /home/osheakes/Research_Project_MMM/Fasta/TEST/CD47/CD47_data.csv\n",
      "All files have identical headers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "results_file_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/processed_blasts.csv\"\n",
    "peptides_file_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/comp_viral_peptide_results.csv\"\n",
    "nrrs_hits_file_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/NRRS_hits.csv\"\n",
    "cd47_data_file_path = \"/home/osheakes/Research_Project_MMM/Fasta/TEST/CD47/CD47_data.csv\"\n",
    "combined_file_path  = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/combined_blasts.csv\"\n",
    "\n",
    "# List of file paths\n",
    "file_paths = [results_file_path, peptides_file_path, nrrs_hits_file_path, cd47_data_file_path]\n",
    "\n",
    "# Dictionary to store headers\n",
    "headers = {}\n",
    "\n",
    "# Read headers\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path, nrows=0)\n",
    "    headers[file_path] = list(df.columns)\n",
    "\n",
    "# Print headers\n",
    "for file_path, header in headers.items():\n",
    "    print(f\"Headers for {file_path}:\")\n",
    "    print(header)\n",
    "    print()\n",
    "\n",
    "# Compare headers\n",
    "identical_files = []\n",
    "non_identical_files = []\n",
    "\n",
    "# Compare each pair of headers\n",
    "for i in range(len(file_paths)):\n",
    "    for j in range(i + 1, len(file_paths)):\n",
    "        file1, file2 = file_paths[i], file_paths[j]\n",
    "        if headers[file1] == headers[file2]:\n",
    "            identical_files.append((file1, file2))\n",
    "        else:\n",
    "            non_identical_files.append((file1, file2))\n",
    "\n",
    "# Print comparison results\n",
    "if identical_files:\n",
    "    print(\"Files with identical headers:\")\n",
    "    for file1, file2 in identical_files:\n",
    "        print(f\"{file1} and {file2}\")\n",
    "else:\n",
    "    print(\"No files with identical headers.\")\n",
    "\n",
    "if non_identical_files:\n",
    "    print(\"\\nFiles with non-identical headers:\")\n",
    "    for file1, file2 in non_identical_files:\n",
    "        print(f\"{file1} and {file2}\")\n",
    "else:\n",
    "    print(\"All files have identical headers.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Add data from RefSeq and Non-Redundancy Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been successfully concatenated.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths to the input CSV files\n",
    "results_file_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/processed_blast/processed_blasts.csv\"\n",
    "peptides_file_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/comp_viral_peptide_results.csv\"\n",
    "nrrs_hits_file_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/viral_peptides/processed_viral_peptides/NRRS_hits.csv\"\n",
    "cd47_data_file_path = \"/home/osheakes/Research_Project_MMM/Fasta/TEST/CD47/CD47_data.csv\"\n",
    "\n",
    "# Path to the output CSV file\n",
    "output_csv_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/final_viral_results/combined_viral_blast_results.csv\"\n",
    "\n",
    "# Function to append the content of one CSV file to another\n",
    "def append_csv(input_file_path, output_file_path, write_header, hit_id_tracker):\n",
    "    with open(input_file_path, 'r', newline='') as input_csvfile:\n",
    "        reader = csv.reader(input_csvfile)\n",
    "        try:\n",
    "            header = next(reader)\n",
    "            hit_id_index = header.index(\"Hit ID\")\n",
    "            accession_id_index = header.index(\"Accession ID\")  \n",
    "        except StopIteration:\n",
    "            # The file is empty, skip it\n",
    "            print(f\"Ignoring file: {input_file_path} - File is empty\")\n",
    "            return\n",
    "        \n",
    "        with open(output_file_path, 'a', newline='') as output_csvfile:\n",
    "            writer = csv.writer(output_csvfile)\n",
    "            \n",
    "            if write_header:\n",
    "                writer.writerow(header)\n",
    "            \n",
    "            for row in reader:\n",
    "                hit_id = row[hit_id_index]\n",
    "                accession_id = row[accession_id_index]\n",
    "                hit_id_tracker[hit_id].append(accession_id)\n",
    "                writer.writerow(row)\n",
    "\n",
    "# Clear the output file if it exists (to avoid appending to an old file)\n",
    "open(output_csv_path, 'w').close()\n",
    "\n",
    "# Track \"Hit ID\" occurrences\n",
    "hit_id_tracker = defaultdict(list)\n",
    "\n",
    "# Append the contents of the input files to the output file\n",
    "append_csv(results_file_path, output_csv_path, write_header=True, hit_id_tracker=hit_id_tracker)\n",
    "append_csv(peptides_file_path, output_csv_path, write_header=False, hit_id_tracker=hit_id_tracker)\n",
    "append_csv(nrrs_hits_file_path, output_csv_path, write_header=False, hit_id_tracker=hit_id_tracker)\n",
    "append_csv(cd47_data_file_path, output_csv_path, write_header=False, hit_id_tracker=hit_id_tracker)\n",
    "\n",
    "\n",
    "\n",
    "print(\"CSV files have been successfully concatenated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Filter to Remove duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed from /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/final_viral_results/combined_viral_blast_results.csv. Filtered file saved to /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/final_viral_results/filtered_combined_viral_blast_results.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def remove_duplicates(input_file_path, output_file_path):\n",
    "    seen_hits = set()\n",
    "    duplicates_found = False\n",
    "    \n",
    "    with open(input_file_path, 'r', newline='') as input_csvfile:\n",
    "        reader = csv.reader(input_csvfile)\n",
    "        header = next(reader)\n",
    "        \n",
    "        # Determine column indices\n",
    "        hit_id_index = header.index(\"Hit ID\")\n",
    "        database_index = header.index(\"Database\")\n",
    "        \n",
    "        # Track duplicates to print them\n",
    "        hit_id_tracker = defaultdict(list)\n",
    "        rows = []  # To store all rows\n",
    "        \n",
    "        for row in reader:\n",
    "            rows.append(row)  # Store the row\n",
    "            hit_id = row[hit_id_index]\n",
    "            database = row[database_index]\n",
    "            accession_id = row[header.index(\"Accession ID\")] \n",
    "            \n",
    "            # Add to hit_id_tracker\n",
    "            hit_id_tracker[(hit_id, database)].append(accession_id)\n",
    "            \n",
    "            # Check for duplicates\n",
    "            if (hit_id, database) in seen_hits:\n",
    "                duplicates_found = True\n",
    "            else:\n",
    "                seen_hits.add((hit_id, database))\n",
    "    \n",
    "    # Rewrite the file without duplicates\n",
    "    if duplicates_found:\n",
    "        with open(output_file_path, 'w', newline='') as output_csvfile:\n",
    "            writer = csv.writer(output_csvfile)\n",
    "            writer.writerow(header)  # Write header\n",
    "            \n",
    "            # Write rows that are not duplicates\n",
    "            for row in rows:\n",
    "                hit_id = row[hit_id_index]\n",
    "                database = row[database_index]\n",
    "                if (hit_id, database) in seen_hits:\n",
    "                    writer.writerow(row)\n",
    "                    seen_hits.remove((hit_id, database))  # Ensure we don't write the same hit again\n",
    "\n",
    "        print(f\"Duplicates removed from {input_file_path}. Filtered file saved to {output_file_path}.\")\n",
    "    else:\n",
    "        print(f\"No duplicates found in {input_file_path}. No changes made.\")\n",
    "\n",
    "# Paths to input and output CSV files\n",
    "input_csv_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/final_viral_results/combined_viral_blast_results.csv\"\n",
    "output_csv_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/final_viral_results/filtered_combined_viral_blast_results.csv\"\n",
    "\n",
    "# Remove duplicates\n",
    "remove_duplicates(input_csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Change Some Accession ID's to fit the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accession IDs updated and CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "##### import pandas as pd\n",
    "\n",
    "# Path to the input CSV file\n",
    "input_csv_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/final_viral_results/combined_viral_blast_results.csv\"\n",
    "\n",
    "# Path to the output CSV file\n",
    "output_csv_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/final_viral_results/combined_viral_blast_results.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Replace Accession IDs based on specified conditions\n",
    "df['Accession ID'].replace({\n",
    "    'Q15303_1': 'Q15303',\n",
    "    'Q15303_1_refseq': 'Q15303',\n",
    "    'Q15303_1_nr': 'Q15303',\n",
    "    'Q15502_1_refseq': 'Q15502',\n",
    "    'P15502_1_refseq': 'P15502',\n",
    "    'Q08722_processed': 'Q08722',\n",
    "    'Q10589_1_refseq': 'Q10589', \n",
    "    'P42574_1': 'P42574', \n",
    "    'P42574_1_nr': 'P42574', \n",
    "    'P42574_1_refseq': 'P42574', \n",
    "    'Q03252_1_refseq': 'Q03252', \n",
    "    'P02533_1_refseq': 'P02533', \n",
    "    'Q96BS2_1_refseq': 'Q96BS2', \n",
    "    'Q99988_1_nr': 'Q99988', \n",
    "    'Q99988_1_refseq': 'Q99988', \n",
    "    'P01570_1_refseq': 'P01570', \n",
    "    'P52630_1_refseq': 'P52630'\n",
    "    \n",
    "    \n",
    "}, inplace=True)\n",
    "\n",
    "# Write the updated DataFrame to a new CSV file\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"Accession IDs updated and CSV file created successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Clean up Accession IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved to /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/final_viral_results/filtered_combined_viral_blast_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/final_viral_results/filtered_combined_viral_blast_results.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove any characters following an underscore in the \"Accession ID\" column\n",
    "df['Accession ID'] = df['Accession ID'].str.split('_').str[0]\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "output_file_path = '/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/final_viral_results/filtered_combined_viral_blast_results.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Processed file saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Add Protein Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/protein_names.txt to CSV format at /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/protein_names.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/protein_names.txt\"\n",
    "output_file = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/protein_names.csv\"\n",
    "\n",
    "# Open the input text file with the appropriate encoding\n",
    "with open(input_file, 'r', encoding='latin-1') as txt_file:\n",
    "    # Read all lines from the text file\n",
    "    lines = txt_file.readlines()\n",
    "\n",
    "# Process the lines to split data and prepare for CSV writing\n",
    "data = []\n",
    "for line in lines:\n",
    "    # Assuming each line has data separated by a specific delimiter, for example, tab or space\n",
    "    # Split each line based on the delimiter and strip any extra whitespace\n",
    "    line_data = line.strip().split('\\t')  \n",
    "    data.append(line_data)\n",
    "\n",
    "# Write the processed data to CSV file\n",
    "with open(output_file, 'w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Converted {input_file} to CSV format at {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file updated successfully at: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/final_viral_results/filtered_combined_viral_processed_blast.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to the CSV files\n",
    "protein_names_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/protein_names.csv\"\n",
    "combined_blast_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/final_viral_results/filtered_combined_viral_blast_results.csv\"\n",
    "output_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/final_viral_results/filtered_combined_viral_processed_blast.csv\"\n",
    "\n",
    "# Read the CSV files\n",
    "protein_names_df = pd.read_csv(protein_names_path)\n",
    "processed_blast_df = pd.read_csv(combined_blast_path)\n",
    "\n",
    "# Ensure 'Accession ID' is of the same type in both DataFrames\n",
    "protein_names_df['Accession ID'] = protein_names_df['Accession ID'].astype(str)\n",
    "processed_blast_df['Accession ID'] = processed_blast_df['Accession ID'].astype(str)\n",
    "\n",
    "# Perform a left merge to add the protein names and types to the processed_blast_df\n",
    "merged_df = pd.merge(processed_blast_df, protein_names_df[['Accession ID', 'Protein Type', 'Protein Names']], on=\"Accession ID\", how=\"left\")\n",
    "\n",
    "# Reorder columns (assuming 'Protein Type' should come first)\n",
    "columns = ['Protein Type', 'Protein Names'] + [col for col in processed_blast_df.columns if col not in ['Protein Type', 'Protein Names']]\n",
    "merged_df = merged_df[columns]\n",
    "\n",
    "# Write the merged dataframe back to a CSV file\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"CSV file updated successfully at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P19838\n",
      "P42574\n",
      "P43490\n",
      "P55265\n",
      "O95786\n",
      "Q8NFJ6\n",
      "P20591\n",
      "O43808\n",
      "P10645\n",
      "P11802\n",
      "P32247\n",
      "Q00535\n",
      "P38398\n",
      "P19525\n",
      "P01344\n",
      "Q15303\n",
      "Q7Z2W4\n",
      "P46663\n",
      "O14879\n",
      "Q00526\n",
      "P60484\n",
      "P01019\n",
      "P55210\n",
      "P24941\n",
      "P25445\n",
      "Q08722\n",
      "Q16611\n",
      "P47898\n",
      "Q07812\n",
      "Q14765\n",
      "P06396\n",
      "P01966\n",
      "P06493\n",
      "Q02388\n",
      "Q12965\n",
      "O95390\n",
      "Q00534\n",
      "Total number of unique Accession IDs: 37\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/final_viral_results/filtered_combined_viral_processed_blast.csv\"\n",
    "\n",
    "# Initialize a set to store unique Accession IDs\n",
    "unique_accession_ids = set()\n",
    "\n",
    "try:\n",
    "    # Open and read the CSV file\n",
    "    with open(csv_file_path, mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        \n",
    "        # Loop through each row in the CSV file\n",
    "        for row in csv_reader:\n",
    "            # Extract the Accession ID and add it to the set\n",
    "            accession_id = row['Accession ID']  \n",
    "            unique_accession_ids.add(accession_id)\n",
    "    \n",
    "    # Print each unique Accession ID\n",
    "    for accession_id in unique_accession_ids:\n",
    "        print(accession_id)\n",
    "    \n",
    "    # Print the total number of unique Accession IDs\n",
    "    print(f\"Total number of unique Accession IDs: {len(unique_accession_ids)}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{csv_file_path}' not found.\")\n",
    "except KeyError:\n",
    "    print(\"Error: 'Accession ID' column not found in the CSV file. Please check the column name.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 Check Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output CSV file contains 5573 rows.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Path to the output CSV file\n",
    "output_csv_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/final_viral_results/combined_viral_processed_blast.csv\"\n",
    "\n",
    "# Function to count the number of rows in a CSV file\n",
    "def count_rows(csv_file_path):\n",
    "    with open(csv_file_path, 'r', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        row_count = sum(1 for row in reader) - 1  # Subtract 1 to exclude the header row\n",
    "    return row_count\n",
    "\n",
    "# Count the rows in the output CSV file\n",
    "row_count = count_rows(output_csv_path)\n",
    "\n",
    "# Print the number of rows\n",
    "print(f\"The output CSV file contains {row_count} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Discern Between MAG and Non-MAG samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'Yes': 285\n",
      "Number of 'No': 5268\n",
      "The updated file has been saved to /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/final_viral_results/filtered_combined_viral_processed_blast.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the absolute file path\n",
    "csv_path = \"/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/final_viral_results/filtered_combined_viral_processed_blast.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "try:\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Ensure the 'Query Definition' column exists\n",
    "    if 'Query Definition' in df.columns:\n",
    "        # Create the new column \"Mags?\" based on the condition\n",
    "        df['Mags?'] = df['Query Definition'].apply(lambda x: 'Yes' if isinstance(x, str) and 'MAG' in x else 'No')\n",
    "\n",
    "        # Print the counts of \"Yes\" and \"No\" values in the \"Mags?\" column\n",
    "        yes_count = df['Mags?'].value_counts().get('Yes', 0)\n",
    "        no_count = df['Mags?'].value_counts().get('No', 0)\n",
    "        print(f\"Number of 'Yes': {yes_count}\")\n",
    "        print(f\"Number of 'No': {no_count}\")\n",
    "\n",
    "        # Save the updated DataFrame back to the original CSV file\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        print(f\"The updated file has been saved to {csv_path}\")\n",
    "    else:\n",
    "        print(\"The 'Query Definition' column was not found in the dataset.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file at {csv_path} was not found. Please check the path and try again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Taxonkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-19 20:41:35--  https://github.com/shenwei356/taxonkit/releases/download/v0.10.1/taxonkit_linux_amd64.tar.gz\n",
      "Resolving github.com (github.com)... 4.208.26.197\n",
      "Connecting to github.com (github.com)|4.208.26.197|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/72552525/a26981d0-259d-4d86-96a1-b64b07b2c9b3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240619%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240619T194135Z&X-Amz-Expires=300&X-Amz-Signature=be3a0d40073e5c8e4175c2f58bc47801410b46d41d9054618300ee2f9b7fc13d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=72552525&response-content-disposition=attachment%3B%20filename%3Dtaxonkit_linux_amd64.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-06-19 20:41:35--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/72552525/a26981d0-259d-4d86-96a1-b64b07b2c9b3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240619%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240619T194135Z&X-Amz-Expires=300&X-Amz-Signature=be3a0d40073e5c8e4175c2f58bc47801410b46d41d9054618300ee2f9b7fc13d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=72552525&response-content-disposition=attachment%3B%20filename%3Dtaxonkit_linux_amd64.tar.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2735258 (2.6M) [application/octet-stream]\n",
      "Saving to: ‘taxonkit_linux_amd64.tar.gz’\n",
      "\n",
      "taxonkit_linux_amd6 100%[===================>]   2.61M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2024-06-19 20:41:36 (71.1 MB/s) - ‘taxonkit_linux_amd64.tar.gz’ saved [2735258/2735258]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/shenwei356/taxonkit/releases/download/v0.10.1/taxonkit_linux_amd64.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxonkit\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf taxonkit_linux_amd64.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/species/taxdump.tar.gz: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!gunzip /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/species/taxdump.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/species/taxdump.tar: Cannot open: No such file or directory\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf /home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq/species/taxdump.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Taxonkit to species list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/osheakes/Research_Project_MMM/Fasta/Viral_Blasts/processed_viral_seq\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for osheakes: \n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install taxonkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 process_species.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
